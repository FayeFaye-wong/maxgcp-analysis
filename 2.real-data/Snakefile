"""MEGASTROKE comparison

This analysis compares two methods for defining phenotypes (naive and MaxGCP).
We'll compute GWAS summary statistics for several variations of these two
methods, then compare their ability to replicate MEGASTROKE GWAS results.
"""
import pathlib
import re

import numpy as np
import pandas as pd
import polars as pl


geno_path = (
    # "/data1/home/mnz2108/"  # mimir
    "/data2/michael/"    # eir
    "data_resources/ukbiobank/hapmap3_genotypes/hapmap3_variants_white_british"
)

feature_phenotypes = {
    "cerebral_infarction": "b_I63",
    "acute_myocardial_infarction": "b_I21",
    "essential_hypertension": "b_I10",
    "obesity": "b_E66",
    "acute_ischemic_heart_disease": "b_I24",
    "ischemic_heart_disease": "b_I25",
    "pulmonary_heart_diseases": "b_I27",
    "other_heart_disease": "b_I51",
    "other_cerebrovascular": "b_I67",
    "dm1": "b_E10",
    "dm2": "b_E11",
    "hba1c": "q_30750_0",
    "hdl_cholesterol": "q_30760_0",
    "glucose": "q_30740_0",
    "cholesterol": "q_30690_0",
    "triglycerides": "q_30870_0",
    "lipoprotein_a": "q_30790_0",
    "apolipoprotein_b": "q_30640_0",
    "creatinine": "q_30700_0"
}
features = sorted(feature_phenotypes.values())
sub_features = [f"sub_{f}" for f in features]
stroke = "b_I63"


rule all:
  input:
    "data/pheno/naive.tsv",
    expand("data/gwas/naive.{pheno}.glm.linear", pheno=features + sub_features),
    "data/gwas/naive.log",
    "data/pcov/naive_full.tsv",
    "data/pcov/naive_sub.tsv",
    expand(
        "data/gwas/MEGASTROKE.{type}.EUR.ldak",
        type=["1.AS", "2.AIS", "3.LAS", "4.CES", "5.SVS"]
    ),
    expand(
        "data/rg/MEGASTROKE.{type}.naive.{pheno}.cors",
        type=["1.AS", "2.AIS", "3.LAS", "4.CES", "5.SVS"],
        pheno=features + sub_features
    ),
    "data/pheno/maxgcp.tsv",
    "data/gwas/maxgcp.log",


rule pick_initial_phenotypes:
  input:
    "../data/pheno/pheno_jan2024.tsv",
  output:
    "data/pheno/naive.tsv",
  run:
    pheno_df = (
        pl.read_csv(input[0], separator="\t", columns=["#FID", "IID"] + features)
        .select(["#FID", "IID"] + features)
    )
    case_df = pheno_df.filter(pl.col(stroke).eq(3))
    control_df = pheno_df.filter(pl.col(stroke).eq(2)).sample(n=len(case_df), seed=0)
    subsampled_df = (
        pl.concat([case_df, control_df])
        .rename(lambda x: x if x in {"#FID", "IID"} else f"sub_{x}")
    )
    (
        pheno_df
        .join(subsampled_df, how="left", on=["#FID", "IID"])
        .rename({"#FID": "FID"})
        .write_csv(output[0], separator="\t", null_value="NA")
    )


rule gwas_naive:
  input:
    pheno = "data/pheno/naive.tsv",
    covar = "../data/pheno/covar.tsv",
    geno = multiext(geno_path, ".pgen", ".psam", ".pvar"),
  output:
    expand("data/gwas/naive.{pheno}.glm.linear", pheno=features + sub_features),
    "data/gwas/naive.log",
  params:
    geno_prefix = geno_path,
    output_prefix = "data/gwas/naive",
  threads: 55
  shell:
    """
    plink2 --pfile {params.geno_prefix} --pheno {input.pheno} \
      --covar {input.covar} --glm hide-covar --threads {threads} \
      --out {params.output_prefix}
    """


rule compute_phenotypic_covariance:
  input:
    pheno = "data/pheno/naive.tsv",
    covar = "../data/pheno/covar.tsv",
  output:
    full = "data/pcov/naive_full.tsv",
    sub = "data/pcov/naive_sub.tsv",
  run:
    pheno_df = pl.read_csv(input.pheno, separator="\t", null_values=["NA"]).drop("FID")
    covar_df = pl.read_csv(input.covar, separator="\t").drop("#FID")
    covariates = covar_df.drop("IID").columns
    merged_df = pheno_df.join(covar_df, on=["IID"])
    # Full phenotypes
    X = merged_df.select(covariates, const=1).to_numpy()
    Y = merged_df.select(features).to_numpy()
    beta = np.linalg.lstsq(X, Y, rcond=None)[0]
    residuals = Y - X @ beta
    cov = np.cov(residuals, rowvar=False)
    (
        pd.DataFrame(cov, index=features, columns=features)
        .to_csv(output.full, sep="\t")
    )
    # Subsampled phenotypes
    merged_df = (
        merged_df
        .select(["sub_" + f for f in features] + covariates)
        .drop_nulls()
    )
    X = merged_df.select(covariates, const=1).to_numpy()
    Y = merged_df.select(sub_features).to_numpy()
    beta = np.linalg.lstsq(X, Y, rcond=None)[0]
    residuals = Y - X @ beta
    cov = np.cov(residuals, rowvar=False)
    (
        pd.DataFrame(cov, index=sub_features, columns=sub_features)
        .to_csv(output.sub, sep="\t")
    )


rule format_megastroke_gwas:
  input:
    gwas = "../data/megastroke/MEGASTROKE.{type}.EUR.out",
    variant_map = (
        "/data2/michael/data_resources/ukbiobank/hapmap3_genotypes/"
        "hapmap3_variants_map.txt"
    ),
  output:
    "data/gwas/MEGASTROKE.{type}.EUR.ldak",
  run:
    gwas_df = pl.read_csv(input.gwas, separator=" ")
    map_df = pl.read_csv(input.variant_map, separator=" ", has_header=False,
                         new_columns=["Predictor", "rsid"]).unique()
    (
        gwas_df
        .join(map_df, left_on=["MarkerName"], right_on=["rsid"])
        .select(
            "Predictor",
            A1=pl.col("Allele1").str.to_uppercase(),
            A2=pl.col("Allele2").str.to_uppercase(),
            n=40_585+406_111,
            Z=pl.col("Effect").truediv(pl.col("StdErr")),
        )
        .write_csv(output[0], separator=" ", float_precision=6)
    )


rule format_plink_gwas:
  input:
    "data/gwas/{pheno}.glm.linear"
  output:
    "data/gwas/{pheno}.glm.linear.ldak"
  shell:
    "sumher_rs fmt -g {input} -o {output}"



rule compute_genetic_correlation:
  input:
    mega = "data/gwas/MEGASTROKE.{type}.EUR.ldak",
    naive = "data/gwas/naive.{pheno}.glm.linear.ldak",
  output:
    "data/rg/MEGASTROKE.{type}.naive.{pheno}.cors",
  threads: 1
  params:
    output_prefix = "data/rg/MEGASTROKE.{type}.naive.{pheno}",
  shell:
    """
    ldak5.2.linux \
      --summary {input.mega} \
      --summary2 {input.naive} \
      --sum-cors {params.output_prefix} \
      --check-sums NO --cutoff 0.01 \
      --tagfile /data2/michael/data_resources/ldak/ldak.thin.hapmap.gbr.tagging
    """


rule collect_genetic_correlations:
  input:
    files = collect(
        "data/rg/MEGASTROKE.{type}.naive.{pheno}.cors",
        type=["1.AS", "2.AIS", "3.LAS", "4.CES", "5.SVS"],
        pheno=features + sub_features
    ),
  output:
    "data/gcov/MEGASTROKE.naive.tsv",
  run:
    dfs = list()
    for path in input.files:
        path = pathlib.Path(path)
        stem = path.stem
        mega_type = re.search("(?<=MEGASTROKE\.).+(?=\.naive)", stem).group()
        pheno_id = re.search("(?<=naive\.).+$", stem).group()
        df = (
            pd.read_csv(path, sep="\s+")
            .assign(
                megastroke_phenotype=mega_type,
                phenotype_id=pheno_id
            )
        )
        dfs.append(df)

    pd.concat(dfs).to_csv(output[0], sep="\t", index=False)


rule compute_maxgcp:
  input:
    pheno = "data/pheno/naive.tsv",
    gcov = "data/gcov/MEGASTROKE.naive.tsv",
    pcov_full = "data/pcov/naive_full.tsv",
    pcov_sub = "data/pcov/naive_sub.tsv",
  output:
    pheno = "data/pheno/maxgcp.tsv",
  run:
    pheno_df = pl.read_csv(input.pheno, separator="\t", null_values=["NA"])
    gcov_df = pl.read_csv(input.gcov, separator="\t")
    pcov_dfs = {
        "full": pd.read_csv(input.pcov_full, sep="\t", index_col=0),
        "sub": pd.read_csv(input.pcov_sub, sep="\t", index_col=0)
    }

    gcov_filtered_df = (
        gcov_df
        # Apply heritability QC filter
        .filter(pl.col("Component").eq("Her2_All"))
        .filter(pl.col("Value").ge(0) & pl.col("Value").le(1))
        .select("phenotype_id")
        .unique()
        # Apply genetic correlation QC filter
        .join(gcov_df, on="phenotype_id")
        .filter(pl.col("Component").eq("Cor_All"))
        .filter(pl.col("Value").ge(-1) & pl.col("Value").le(1))
        .select("phenotype_id", "megastroke_phenotype")
        .unique()
        .join(gcov_df, on=["phenotype_id", "megastroke_phenotype"])
        .filter(pl.col("Component").eq("Coher_All"))
        .drop(["Component"])
        .with_columns(
            cohort=pl.when(pl.col("phenotype_id").str.contains("sub"))
                .then(pl.lit("sub")).otherwise(pl.lit("full"))
        )
        .to_pandas()
    )

    maxgcp_df = list()
    groups = gcov_filtered_df.groupby(["cohort", "megastroke_phenotype"])
    for (cohort, mega_type), group_df in groups:
        gcov_vec = (
            group_df
            .set_index("phenotype_id")
            .loc[:, "Value"]
        )
        index = gcov_vec.index
        sub_pcov_df = pcov_dfs[cohort].loc[index, index]
        beta = np.linalg.lstsq(sub_pcov_df, gcov_vec, rcond=None)[0]
        beta /= beta @ sub_pcov_df @ beta
        beta_vec = pd.Series(beta, index=index)

        feature_df = pheno_df.to_pandas().set_index(["FID", "IID"]).loc[:, index]
        maxgcp_vec = feature_df @ beta_vec
        maxgcp_df.append(maxgcp_vec.rename(f"maxgcp_{cohort}_{mega_type}"))

    maxgcp_df = (
        pd.concat(maxgcp_df, axis=1, ignore_index=False)
        .sort_index(axis=1)
    )
    maxgcp_df.to_csv(output.pheno, sep="\t", na_rep="NA")


rule direct_gwas_maxgcp:
  input:
    pheno = "data/pheno/maxgcp.tsv",
    covar = "../data/pheno/covar.tsv",
    geno = multiext(geno_path, ".pgen", ".psam", ".pvar"),
  output:
    "data/gwas/maxgcp.log",
  params:
    geno_prefix = geno_path,
    output_prefix = "data/gwas/maxgcp",
  threads: 55
  shell:
    """
    plink2 --pfile {params.geno_prefix} --pheno {input.pheno} \
      --covar {input.covar} --glm hide-covar zs --threads {threads} \
      --out {params.output_prefix}
    """
